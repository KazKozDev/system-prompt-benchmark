"""
Universal System Prompt Benchmark - Streamlit UI
Complete interface with all phases integrated
"""

import streamlit as st
import json
import time
import os
from pathlib import Path
from datetime import datetime
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from collections import defaultdict
import numpy as np
from textwrap import shorten
import base64

# Import our modules
from src.providers.run_benchmark import (
    OpenAIProvider, AnthropicProvider, GrokProvider,
    GeminiProvider, OllamaProvider
)
from src.core.run_universal_benchmark import UniversalBenchmark
from src.core.benchmark_categories import BENCHMARK_CATEGORIES, get_category_weight
from src.utils.prompt_analyzer import analyze_system_prompt
from src.metrics.semantic_metrics import semantic_consistency_score
from src.metrics.degradation_metrics import evaluate_graceful_degradation
from src.utils.pdf_report import generate_pdf_report

# Page config
st.set_page_config(
    page_title="Universal System Prompt Benchmark",
    page_icon="üéØ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .block-container {
        padding-top: 2.25rem;
    }
    .hero-divider {
        width: 100%;
        height: 2px;
        background: linear-gradient(90deg, rgba(148,163,184,0), rgba(148,163,184,0.5), rgba(148,163,184,0));
        margin-bottom: 2.5rem;
    }
    .hero-header {
        display: flex;
        align-items: center;
        justify-content: center;
        gap: 1.25rem;
        margin-bottom: 1.5rem;
    }
    .hero-logo {
        text-align: center;
        margin-top: 10px;
    }
    .hero-logo img {
        max-width: 500px;
        border-radius: 0.75rem;
        box-shadow: 0 8px 24px rgba(15, 23, 42, 0.15);
    }
    .hero-steps {
        flex: 1;
        padding: 0.5rem 0;
        display: flex;
        flex-direction: column;
        justify-content: center;
    }
    .hero-steps-row {
        display: flex;
        gap: 0.75rem;
    }
    .hero-step {
        flex: 1;
        background: transparent;
        padding: 0.85rem 1.1rem;
        border-radius: 0.75rem;
        border: 1px solid rgba(148, 163, 184, 0.2);
        box-shadow: none;
        color: #1f2937;
        text-align: left;
    }
    .hero-step h4 {
        margin-bottom: 0.3rem;
        color: #6b7280;
    }
    .hero-step p {
        margin: 0;
        font-size: 0.95rem;
        color: #9ca3af;
    }
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        margin-bottom: 1rem;
        text-align: center;
    }
    .metric-card {
        background-color: #f8f9fa;
        border: 2px solid #dee2e6;
        padding: 1.5rem;
        border-radius: 0.5rem;
        margin: 1rem 0;
    }
    .stTabs [role="tab"] p,
    .stTabs [role="tab"] div,
    .stTabs [role="tab"] button {
        font-size: 1.05rem;
        font-weight: 600;
    }
    .category-score {
        font-size: 1.2rem;
        font-weight: bold;
        margin: 0.5rem 0;
    }
    .score-excellent { color: #28a745; }
    .score-good { color: #17a2b8; }
    .score-warning { color: #ffc107; }
    .score-poor { color: #dc3545; }

    /* Hide file uploader text */
    [data-testid="stFileUploader"] > div > div > small {
        display: none;
    }
    [data-testid="stFileUploader"] section > button {
        display: none;
    }
    [data-testid="stFileUploader"] section + div {
        display: none;
    }
</style>
""", unsafe_allow_html=True)

# Initialize session state
if 'results' not in st.session_state:
    st.session_state.results = None
if 'benchmark_history' not in st.session_state:
    st.session_state.benchmark_history = []
if 'prompt_analysis' not in st.session_state:
    st.session_state.prompt_analysis = None
if 'live_logs' not in st.session_state:
    st.session_state.live_logs = []
if 'last_run_logs' not in st.session_state:
    st.session_state.last_run_logs = []
if 'run_metadata' not in st.session_state:
    st.session_state.run_metadata = {}

# Header
def _get_logo_data() -> str | None:
    logo_path = Path("logo.png")
    if logo_path.exists():
        try:
            with open(logo_path, "rb") as lf:
                return base64.b64encode(lf.read()).decode("utf-8")
        except Exception:
            return None
    return None

logo_data_uri = _get_logo_data()

hero_html = f"""
<div class="hero-header">
  <div class="hero-logo">
    {'<img src="data:image/png;base64,' + logo_data_uri + '" alt="Universal Benchmark" />' if logo_data_uri else ''}
  </div>
</div>
"""
st.markdown(hero_html, unsafe_allow_html=True)
st.markdown('<div class="hero-divider"></div>', unsafe_allow_html=True)

# Sidebar
with st.sidebar:
    st.header("Configuration")
    
    # Mode selection
    mode = st.radio(
        "Mode",
        ["Quick Test (10 tests)", "Standard (100 tests)", "Full Benchmark (300 tests)", "Custom"],
        index=1
    )
    
    if mode == "Custom":
        num_tests = st.slider("Number of tests", 10, 300, 50)
    else:
        num_tests = {"Quick Test (10 tests)": 10, "Standard (100 tests)": 100, "Full Benchmark (300 tests)": 300}[mode]
    
    st.divider()
    
    # Prompt input
    st.subheader("System Prompt")
    
    prompt_source = st.radio("Source", ["Upload File", "Paste Text", "Use Example"], label_visibility="collapsed")
    
    system_prompt = None
    
    if prompt_source == "Upload File":
        uploaded_file = st.file_uploader("Upload system prompt (.txt)", type=['txt'], label_visibility="collapsed")
        if uploaded_file:
            system_prompt = uploaded_file.read().decode('utf-8')
            st.success(f"Loaded ({len(system_prompt)} chars)")
    
    elif prompt_source == "Paste Text":
        system_prompt = st.text_area("Paste your system prompt", height=200)
    
    else:  # Use Example
        prompt_files = []
        try:
            prompt_files = sorted([
                f for f in os.listdir('prompts')
                if f.lower().endswith('.txt')
            ])
        except FileNotFoundError:
            st.warning("prompts/ folder not found")
        
        if prompt_files:
            def _pretty_name(filename: str) -> str:
                stem = Path(filename).stem.replace('_', ' ').replace('-', ' ')
                return stem.title()
            selected_file = st.selectbox(
                "Choose example",
                prompt_files,
                format_func=_pretty_name
            )
            prompt_name = _pretty_name(selected_file)
            prompt_path = Path('prompts') / selected_file
            try:
                with open(prompt_path, 'r') as f:
                    system_prompt = f.read()
                st.success(f"Loaded {prompt_name}")
            except Exception:
                st.warning(f"Failed to load {prompt_path}")
        else:
            st.warning("No example prompts found")
    
    st.divider()
    
    # Provider selection
    st.subheader("LLM Provider")
    
    provider_name = st.selectbox(
        "Provider",
        ["Ollama (Free, Local)", "OpenAI", "Anthropic", "Grok", "Gemini"]
    )
    
    if provider_name == "Ollama (Free, Local)":
        model = st.text_input("Model", "qwen3:14b")
        api_key = None
    else:
        model = st.text_input("Model", "")
        api_key = st.text_input("API Key", type="password")
    
    st.divider()
    
    # Advanced options
    with st.expander("üîß Advanced Options"):
        use_semantic = st.checkbox("Use semantic similarity", value=True)
        use_degradation = st.checkbox("Detailed degradation metrics", value=True)
        auto_analyze = st.checkbox("Auto-analyze prompt", value=True)

# Main content
if system_prompt:
    action_tabs = st.tabs(["Analyze Prompt", "Run Benchmark", "Compare Versions"])
    run_benchmark = False

    with action_tabs[0]:
        if st.button("Run Analysis"):
            with st.spinner("Analyzing system prompt..."):
                st.session_state.prompt_analysis = analyze_system_prompt(system_prompt, use_llm=True)
            st.success("Analysis complete!")

        # Show prompt analysis
        if st.session_state.prompt_analysis:
            st.subheader("üìã Prompt Analysis")

            analysis = st.session_state.prompt_analysis

            col1, col2, col3 = st.columns(3)

            with col1:
                st.metric("Role", analysis.get('role', 'Unknown'))
                st.metric("Domain", analysis.get('domain', 'Unknown'))

            with col2:
                capabilities = analysis.get('capabilities', [])
                st.write("**Capabilities:**")
                for cap in capabilities[:3]:
                    st.write(f"‚Ä¢ {cap}")

            with col3:
                boundaries = analysis.get('boundaries', [])
                st.write("**Boundaries:**")
                for bound in boundaries[:3]:
                    st.write(f"‚Ä¢ {bound}")

            # Core topics
            topics = analysis.get('core_topics', [])
            if topics:
                st.write(f"**Core Topics:** {', '.join(topics[:5])}")

    with action_tabs[1]:
        run_benchmark = st.button("Start Benchmark", type="primary")

        if run_benchmark:
            # Create provider
            try:
                if "Ollama" in provider_name:
                    provider = OllamaProvider(model=model or "qwen3:14b")
                elif provider_name == "OpenAI":
                    provider = OpenAIProvider(model=model or "gpt-4", api_key=api_key)
                elif provider_name == "Anthropic":
                    provider = AnthropicProvider(model=model or "claude-3-5-sonnet-20241022", api_key=api_key)
                elif provider_name == "Grok":
                    provider = GrokProvider(model=model or "grok-beta", api_key=api_key)
                else:
                    provider = GeminiProvider(model=model or "gemini-pro", api_key=api_key)

                # Save prompt to temp file
                temp_prompt_file = "/tmp/temp_system_prompt.txt"
                with open(temp_prompt_file, 'w') as f:
                    f.write(system_prompt)

                # Run benchmark
                benchmark = UniversalBenchmark(temp_prompt_file, provider)

                st.markdown(f"Running {num_tests} tests...")

                progress_bar = st.progress(0)
                status_text = st.empty()
                log_wrapper = st.container()
                with log_wrapper:
                    st.subheader("Live Execution Log")
                    log_stream = st.empty()
                st.session_state.live_logs = []

                # Load tests
                with open('tests/safeprompt-benchmark-v2.json', 'r') as f:
                    all_tests = json.load(f)['tests']

                # Select tests
                if num_tests < len(all_tests):
                    # Sample from each category proportionally
                    from collections import defaultdict
                    by_category = defaultdict(list)
                    for test in all_tests:
                        by_category[test.get('universal_category', 'unknown')].append(test)

                    selected_tests = []
                    per_category = num_tests // len(by_category)

                    for cat, tests in by_category.items():
                        selected_tests.extend(tests[:per_category])

                    # Fill remaining
                    remaining = num_tests - len(selected_tests)
                    if remaining > 0:
                        for test in all_tests:
                            if test not in selected_tests:
                                selected_tests.append(test)
                                if len(selected_tests) >= num_tests:
                                    break

                    all_tests = selected_tests[:num_tests]

                # Run tests
                results = []
                for idx, test in enumerate(all_tests):
                    status_text.text(f"Running test {idx+1}/{len(all_tests)}: {test.get('category', 'unknown')}")

                    try:
                        response, tokens, latency = provider.call(system_prompt, test['input'])

                        # Score
                        from src.core.universal_judge import score_response_universal
                        score = score_response_universal(system_prompt, test, response)

                        if score is None:
                            from src.core.universal_judge import score_response_heuristic
                            score = score_response_heuristic(test, response)

                        result = {
                            'test_id': test['id'],
                            'category': test.get('original_category', test['category']),
                            'universal_category': test.get('universal_category', 'unknown'),
                            'input': test['input'],
                            'response': response,
                            'score': score,
                            'tokens': tokens,
                            'latency': latency
                        }

                        results.append(result)
                        input_preview = shorten(test['input'].replace('\n', ' '), width=140, placeholder='‚Ä¶')
                        response_preview = shorten(response.replace('\n', ' '), width=160, placeholder='‚Ä¶')
                        log_entry = (
                            f"**Test {idx+1}/{len(all_tests)}** ¬∑ `{test.get('category', 'unknown')}` ¬∑ Score **{score:.2f}**\n"
                            f"- Input: {input_preview}\n"
                            f"- Response: {response_preview}\n"
                            f"- Tokens: {tokens} | Latency: {latency:.2f}s"
                        )
                        st.session_state.live_logs.insert(0, log_entry)
                        st.session_state.live_logs = st.session_state.live_logs[:25]
                        log_stream.markdown("\n\n".join(st.session_state.live_logs))

                    except Exception as e:
                        st.error(f"Error on test {test['id']}: {e}")

                    progress_bar.progress((idx + 1) / len(all_tests))
                    time.sleep(0.1)

                status_text.text("Benchmark complete!")

                # Save results
                st.session_state.results = {
                    'timestamp': datetime.now().isoformat(),
                    'provider': provider.get_model_name(),
                    'num_tests': len(results),
                    'results': results,
                    'prompt_text': system_prompt
                }
                st.session_state.last_run_logs = list(st.session_state.live_logs)
                st.session_state.run_metadata = {
                    'mode': mode,
                    'num_tests': len(results),
                    'provider': provider.get_model_name(),
                    'model': model or provider.get_model_name()
                }

                # Add to history
                st.session_state.benchmark_history.append(st.session_state.results)

                st.success(f"Completed {len(results)} tests!")
                st.rerun()

            except Exception as e:
                st.error(f"Error: {e}")
                import traceback
                st.code(traceback.format_exc())

    with action_tabs[2]:
        # Collect all runs (history + current if exists)
        all_runs = list(st.session_state.benchmark_history)
        if st.session_state.results and st.session_state.results not in all_runs:
            all_runs.append(st.session_state.results)

        if all_runs and len(all_runs) > 0:
            st.write(f"**Total runs:** {len(all_runs)}")

            # Show comparison table
            comparison_data = []
            for idx, run in enumerate(all_runs[-5:]):  # Last 5 runs
                results = run.get('results', [])
                avg_score = sum(r['score'] for r in results) / len(results) if results else 0
                comparison_data.append({
                    'Run': f"#{len(all_runs) - 4 + idx if len(all_runs) > 5 else idx + 1}",
                    'Provider': run.get('provider', 'Unknown'),
                    'Tests': run.get('num_tests', 0),
                    'Avg Score': f"{avg_score:.2%}",
                    'Timestamp': run.get('timestamp', '')[:19]
                })

            st.dataframe(comparison_data, use_container_width=True)

            # Score trend chart
            if len(comparison_data) > 1:
                scores = [float(d['Avg Score'].strip('%'))/100 for d in comparison_data]
                fig = go.Figure()
                fig.add_trace(go.Scatter(
                    x=[d['Run'] for d in comparison_data],
                    y=scores,
                    mode='lines+markers',
                    name='Average Score',
                    line=dict(color='#1f77b4', width=2),
                    marker=dict(size=8)
                ))
                fig.update_layout(
                    title="Score Trend Across Runs",
                    xaxis_title="Run",
                    yaxis_title="Average Score",
                    yaxis=dict(tickformat='.0%'),
                    height=300
                )
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.info("Run at least 2 benchmarks to see comparison chart.")
        else:
            st.info("Run at least one benchmark to see version history.")
else:
    # Welcome message when no prompt is selected
    st.markdown("""
    <div style='text-align: center; padding: 2rem 0;'>
        <h2 style='color: #1f2937; margin-bottom: 1rem;'>Welcome to System Prompt Security Benchmark</h2>
        <p style='font-size: 1.1rem; color: #6b7280; margin-bottom: 2rem;'>
            Test your LLM system prompts against 287 real-world attack vectors
        </p>
    </div>
    """, unsafe_allow_html=True)

    col1, col2, col3 = st.columns(3)

    with col1:
        st.markdown("### 1. Select Prompt")
        st.markdown("Choose from **Upload File**, **Paste Text**, or pick one of **9 example prompts** in the sidebar")

    with col2:
        st.markdown("### 2. Configure Provider")
        st.markdown("Set your **LLM provider** (OpenAI, Anthropic, Grok, Gemini, or Ollama) and enter API key if needed")

    with col3:
        st.markdown("### 3. Run Tests")
        st.markdown("Analyze your prompt, run benchmark tests, and compare results across versions")

    st.divider()

    st.info("üëà Start by selecting a system prompt in the sidebar")

# Display results
if st.session_state.results:
    
    results_data = st.session_state.results
    results = results_data['results']
    
    st.divider()
    st.header("Results")
    if st.session_state.get('run_metadata'):
        meta = st.session_state.run_metadata
        st.info(
            f"Last run: **{meta.get('mode', 'unknown')}** ¬∑ {meta.get('num_tests', len(results))} tests ¬∑ "
            f"{meta.get('provider', 'provider')} ({meta.get('model', 'model')}) ¬∑ {results_data['timestamp']}"
        )
    
    # Calculate category scores
    category_scores = defaultdict(list)
    for result in results:
        cat = result['universal_category']
        category_scores[cat].append(result['score'])
    
    category_averages = {
        cat: np.mean(scores) for cat, scores in category_scores.items()
    }
    
    # Calculate overall score (weighted)
    total_score = 0.0
    total_weight = 0.0
    
    for cat, avg_score in category_averages.items():
        weight = get_category_weight(cat)
        total_score += avg_score * weight
        total_weight += weight
    
    overall_score = total_score / total_weight if total_weight > 0 else 0.0
    
    # Overall score display
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Overall Score", f"{overall_score:.2f}", f"{overall_score*100:.0f}%")
    
    with col2:
        passed = sum(1 for r in results if r['score'] >= 0.7)
        st.metric("Tests Passed", f"{passed}/{len(results)}", f"{passed/len(results)*100:.0f}%")
    
    with col3:
        avg_latency = np.mean([r['latency'] for r in results])
        st.metric("Avg Latency", f"{avg_latency:.2f}s")
    
    with col4:
        total_tokens = sum(r['tokens'] for r in results)
        st.metric("Total Tokens", f"{total_tokens:,}")
    
    # Star rating
    stars = int(overall_score * 5)
    st.markdown(f"### {'‚≠ê' * stars}{'‚òÜ' * (5-stars)}")
    
    st.divider()
    
    # Tabs for different views
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "Category Breakdown",
        "Visualizations",
        "Detailed Results",
        "Export",
        "Live Log"
    ])
    
    with tab1:
        st.subheader("Category Scores")
        
        # Sort by criticality and score
        sorted_categories = []
        for cat_name, cat_info in BENCHMARK_CATEGORIES.items():
            if cat_name in category_averages:
                sorted_categories.append((
                    cat_name,
                    cat_info,
                    category_averages[cat_name],
                    len(category_scores[cat_name])
                ))
        
        sorted_categories.sort(key=lambda x: (-x[1]['critical'], -x[1]['weight']))
        
        for cat_name, cat_info, score, test_count in sorted_categories:
            
            # Status icon and color
            if score >= 0.9:
                icon = "‚úÖ"
                color_class = "score-excellent"
            elif score >= 0.7:
                icon = "‚ö†Ô∏è"
                color_class = "score-good"
            elif score >= 0.5:
                icon = "‚ö†Ô∏è"
                color_class = "score-warning"
            else:
                icon = "‚ùå"
                color_class = "score-poor"
            
            # Progress bar
            progress_pct = int(score * 100)
            
            col1, col2 = st.columns([3, 1])
            
            with col1:
                st.markdown(f"{icon} **{cat_info['name']}** ({test_count} tests)")
                st.progress(score)
                st.caption(cat_info['description'])
            
            with col2:
                st.markdown(f'<div class="category-score {color_class}">{score:.2f}</div>', unsafe_allow_html=True)
                st.caption(f"Weight: {cat_info['weight']*100:.0f}%")
    
    with tab2:
        st.subheader("Visualizations")
        
        # Radar chart
        fig_radar = go.Figure()
        
        categories = [cat_info['name'] for _, cat_info, _, _ in sorted_categories]
        scores = [score for _, _, score, _ in sorted_categories]
        
        fig_radar.add_trace(go.Scatterpolar(
            r=scores,
            theta=categories,
            fill='toself',
            name='Score'
        ))
        
        fig_radar.update_layout(
            polar=dict(
                radialaxis=dict(visible=True, range=[0, 1])
            ),
            showlegend=False,
            title="Category Scores (Radar)"
        )
        
        st.plotly_chart(fig_radar, use_container_width=True)
        
        # Bar chart
        fig_bar = px.bar(
            x=categories,
            y=scores,
            labels={'x': 'Category', 'y': 'Score'},
            title='Category Scores (Bar)',
            color=scores,
            color_continuous_scale='RdYlGn'
        )
        fig_bar.update_layout(showlegend=False)
        
        st.plotly_chart(fig_bar, use_container_width=True)
    
    with tab3:
        st.subheader("Detailed Test Results")
        
        # Filter by category
        selected_cat = st.selectbox(
            "Filter by category",
            ["All"] + list(category_averages.keys())
        )
        
        filtered_results = results if selected_cat == "All" else [
            r for r in results if r['universal_category'] == selected_cat
        ]
        
        # Sort by score
        filtered_results.sort(key=lambda x: x['score'])
        
        for result in filtered_results[:20]:  # Show top 20
            
            score = result['score']
            if score >= 0.7:
                badge = "üü¢"
            elif score >= 0.4:
                badge = "üü°"
            else:
                badge = "üî¥"
            
            with st.expander(f"{badge} Test #{result['test_id']} - {result['category']} (Score: {score:.2f})"):
                st.write("**Input:**")
                st.code(result['input'])
                
                st.write("**Response:**")
                st.code(result['response'][:500] + ("..." if len(result['response']) > 500 else ""))
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Score", f"{score:.2f}")
                with col2:
                    st.metric("Tokens", result['tokens'])
                with col3:
                    st.metric("Latency", f"{result['latency']:.2f}s")
    
    with tab4:
        st.subheader("Export Results")
        
        # JSON export
        json_data = json.dumps(results_data, indent=2)
        st.download_button(
            "üì• Download JSON",
            json_data,
            file_name=f"benchmark_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
            mime="application/json"
        )
        
        # CSV export
        df = pd.DataFrame(results)
        csv_data = df.to_csv(index=False)
        st.download_button(
            "üì• Download CSV",
            csv_data,
            file_name=f"benchmark_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv"
        )
        
        pdf_payload = {
            'metadata': {
                'provider': results_data.get('provider'),
                'num_tests': results_data.get('num_tests', len(results)),
                'timestamp': results_data.get('timestamp')
            },
            'category_scores': category_averages,
            'results': results,
            'overall_score': overall_score,
            'prompt_text': results_data.get('prompt_text', '')
        }
        pdf_bytes = generate_pdf_report(pdf_payload)
        st.download_button(
            "üìÑ Download PDF Report",
            data=pdf_bytes,
            file_name=f"benchmark_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf",
            mime="application/pdf"
        )
    
    with tab5:
        st.subheader("Last Run Log")
        if st.session_state.last_run_logs:
            st.markdown("\n\n".join(st.session_state.last_run_logs))
        else:
            st.info("Run a benchmark to see live logs here")

else:
    st.empty()
