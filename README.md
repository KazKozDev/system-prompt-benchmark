# System Prompt Benchmark

A collection of secure system prompts with benchmarking tools to test their safety and effectiveness against prompt injection, jailbreaking, and other attack vectors.

## ğŸ“‹ Overview

This repository contains production-ready system prompts designed with security best practices and a benchmarking framework to evaluate their robustness.

### Who is this for?

- Prompt Engineers
- AI/ML Developers
- Product Managers working with LLMs
- AI Safety Researchers

## ğŸ¯ Prompts Collection

### 1. Customer Support Bot
**File:** `prompts/customer-support-bot.txt` 

System prompt for an e-commerce customer support chatbot.

**Key Features:**
- Handles product and order inquiries
- Protection against role-playing attacks
- User data isolation

**Use Cases:**
- E-commerce support
- SaaS technical support
- Information bots

---

### 2. Social Media Content Creator
**File:** `prompts/social-media-content-creator.txt` 

Prompt for generating social media content for a fitness brand.

**Key Features:**
- Brand voice control
- Content safety filters
- Medical claim filtering

**Use Cases:**
- Social media posts
- Content marketing
- Creative teams

---

### 3. Corporate Knowledge Base RAG
**File:** `prompts/corporate-knowledge-rag.txt` 

Prompt for a RAG system with access control for corporate knowledge bases.

**Key Features:**
- Document-level access control
- Data leakage protection
- Source citation

**Use Cases:**
- Internal knowledge bases
- Company documentation
- FAQ systems

---

### 4. Educational Tutor
**File:** `prompts/educational-tutor.txt` 

Prompt for an educational assistant for math and science.

**Key Features:**
- Academic integrity focus
- Guidance over direct answers
- Student level adaptation

**Use Cases:**
- EdTech applications
- Tutoring platforms
- Learning systems

## ğŸ›¡ï¸ Security Principles

All prompts follow these security principles:

### Protection Against Prompt Injection
- Clear separation of system instructions and user input
- XML tags for structure
- Explicit instructions to ignore embedded commands

### Protection Against Jailbreaking
- Defined behavioral boundaries
- Role-playing attempt handling
- Refusal to execute prohibited actions

### Protection Against Prompt Leaking
- Prohibition on revealing system instructions
- No sensitive data in prompts
- Secure configuration storage

### Data Protection
- User data isolation
- Role-based access control
- Cross-user query prohibition

## ğŸ§ª Benchmarking

### Running the Benchmark
```bash
# Install dependencies
pip install -r requirements.txt

# Set API keys
export OPENAI_API_KEY="your-key"
# or
export ANTHROPIC_API_KEY="your-key"

# Run benchmark
python run_benchmark.py --prompt prompts/customer-support-bot.txt --provider openai
```

### Benchmark Categories

The benchmark tests four categories:

1. **Security Tests** - Prompt injection, jailbreaking, prompt leaking
2. **Functionality Tests** - Normal queries and expected behavior
3. **Boundary Tests** - Out-of-scope request handling
4. **Edge Cases** - Social engineering, multilingual attacks

### Example Results
```
=== BENCHMARK RESULTS ===

Security: 13/13 (100%) âœ…
Functionality: 8/8 (100%) âœ…
Boundaries: 7/7 (100%) âœ…
Edge Cases: 6/6 (100%) âœ…

Overall Score: 34/34 (100%)
Model: GPT-5-1
```

## ğŸ“Š Test Results

### TechStore Customer Support Bot

**Tested on:** GPT-5-1  
**Date:** 2025-11-19

| Category | Score | Status |
|----------|-------|--------|
| Security | 13/13 (100%) | âœ… |
| Functionality | 8/8 (100%) | âœ… |
| Boundaries | 7/7 (100%) | âœ… |
| Edge Cases | 6/6 (100%) | âœ… |
| **Overall** | **34/34 (100%)** | âœ… |

## ğŸš€ Quick Start

### Using a Prompt
```python
# With OpenAI
import openai

with open('prompts/customer-support-bot.txt', 'r') as f:
    system_prompt = f.read()

response = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": "How do I return a product?"}
    ]
)
```
```python
# With Anthropic Claude
import anthropic

with open('prompts/customer-support-bot.txt', 'r') as f:
    system_prompt = f.read()

client = anthropic.Anthropic(api_key="your-key")
message = client.messages.create(
    model="claude-3-5-sonnet-20241022",
    max_tokens=1024,
    system=system_prompt,
    messages=[
        {"role": "user", "content": "How do I return a product?"}
    ]
)
```

## ğŸ“ Repository Structure
```
system-prompt-benchmark/
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ prompts/                    # System prompts
â”‚   â”œâ”€â”€ customer-support-bot.txt
â”‚   â”œâ”€â”€ social-media-content-creator.txt
â”‚   â”œâ”€â”€ corporate-knowledge-rag.txt
â”‚   â””â”€â”€ educational-tutor.txt
â”œâ”€â”€ tests/                      # Benchmark test datasets
â”‚   â””â”€â”€ techstore_benchmark.json
â”œâ”€â”€ run_benchmark.py           # Benchmark script
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ results/                   # Benchmark results
    â””â”€â”€ techstore_results.json
```

## ğŸ”§ Customization

Each prompt can be adapted to your needs:

1. Replace placeholders with your company/product data
2. Add specific rules for your use case
3. Adjust tone and style for your brand
4. Extend boundaries for additional functionality

## ğŸ“– Best Practices

### âœ¨ Recommendations

1. **Never store secrets in prompts** - use environment variables
2. **Log suspicious requests** - analyze attack patterns
3. **Update regularly** - new attack vectors emerge constantly
4. **Test on production data** - synthetic tests don't cover everything
5. **Use rate limiting** - protection against automated attacks

### ğŸš¨ What NOT to Do

- âŒ Don't rely only on prompts for security
- âŒ Don't store API keys in system prompts
- âŒ Don't use one prompt for different access levels
- âŒ Don't ignore logs - they show real attack attempts

## ğŸ¤ Contributing

We welcome contributions!

### Adding a New Prompt

1. Create a new `.txt` file with the prompt
2. Follow the structure of existing prompts
3. Test against adversarial examples
4. Update README with description
5. Create a Pull Request

### Reporting Security Issues

If you find a vulnerability in the prompts:
- Create an issue with the `security` label
- Describe the attack vector
- Propose a solution if possible

## ğŸ“š Resources

### Further Reading

- [OWASP Top 10 for LLM Applications](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
- [Anthropic Prompt Engineering Guide](https://docs.anthropic.com/claude/docs/prompt-engineering)
- [OpenAI Safety Best Practices](https://platform.openai.com/docs/guides/safety-best-practices)

## ğŸ“„ License

MIT License - Free to use in commercial and personal projects.

## â­ Support

If this repository was helpful, please give it a star!

Made with â¤ï¸ for the AI Safety community
